# Necesario si installi gcloud
# Necesario si installi kubectl


# CREAZIONE NUOVO PROGETTO:
# gcloud projects create project_name
# Es.
# gcloud projects create th-prj-20240229

# SET PROJECT
# gcloud config set project th-prj-20240229

# INIT PROJECT
# gcloud init

# SET ZONE E REGIONE
# visualizzazione zone a disposizione
# 1. abilitare fatturazione del servizio API Compute Engine all’interno di google cloud platform (https://console.cloud.google.com/apis/library?project=base-prj-20240117)

# gcloud compute zones list

# 2. si visualizzi la lista delle zone:
#  us-central1-c 
# all'interno della sezione configurazione macchina, presente al link https://gcloud-compute.com/instances.html
# si riscontra, in modo immediato, il costo effettivo, corrispondente alle varie aree geografiche, dell'istanza selezionata.

# 3.set zona:
# gcloud config set compute/zone zone_name
# es.
# gcloud config set compute/zone us-central1-c 

# 4. verifica property:
# gcloud info 

# 5. si visualizzi la lista delle regioni:
# gcloud compute regions list

# 6.set zona regione:
# gcloud config set compute/region nome_regione
# es.
# gcloud config set compute/region us-central1

# 7. verifica property:
# gcloud info 

# # CREAZIONE CLUSTER 
# 1. Abilitazione kubernates Engine API
# 2. Abilitazione container.googleapis.com
# gcloud services enable container.googleapis.com
# 3. Creazione cluster
# to test -> gcloud container clusters create base-prj-20240117- cls--num-nodes 1 --machine-type e2-standard-2 --no-enable-autoscaling --max-pods-per-node 200 --enable-ip-alias
# gcloud container clusters create th-prj-20240229-cls --num-nodes 1 --machine-type e2-standard-2 --no-enable-autoscaling

# # TARAHUMARA PROJECT

# # ************* CREAZIONE NETWORK POLICIES *************
# SI VERIFICHI ATTRAVERSO LA PROPERTY networkPolicyConfig L'ABILITAZIONE DEL CLUSTER ALLE NETWORK POLICIES
# gcloud container clusters describe th-prj-20240229-cls --zone us-central1-c 
# IN CASO DI NETWORK POLICIES NON ABILITATE:
# gcloud container clusters update th-prj-20240229-cls --update-addons=NetworkPolicy=ENABLED --zone us-central1-c 
# QUINDI:
# gcloud container clusters update th-prj-20240229-cls --enable-network-policy

# # ************* CREAZIONE DB *************
# # LISTA CLUSTER
# gcloud container clusters list
# # CREAZIONE DB (name= mysqldb)
# gcloud compute disks create --size=10GiB --zone=us-central1-c th-prj-db
# gcloud compute disks list
# # SI CREI UN PERSISTENT VOLUME, CREIAMO QUINDI SPAZIO
# kubectl create -f persistentVolumes/th-prj-db-pv.yml 
# kubectl get PersistentVolumes
# # SI CREI UN PERSISTENT VOLUME CLAIM, UILIZIAMO QUINDI, LO SPAZIO CREATO CON pv
# kubectl create -f persistentVolumesClaims/th-prj-db-pvc.yml
# kubectl get PersistentVolumeClaim

# SI VERIFICHI IL QUANTITATIVO DI CPU E MEMORIA A DISPOSIZIONE 
# PER DIMENSIONARE COSI' IN MODO CORRETTO I POD. SI VERIFICHINO QUINDI LE PROPERTY Allocatable:  cpu: E memory:
# SI CONSIDERI INOLTRE 1core = 1000millicores = 1000m
# kubectl describe nodes

# # ************* CREAZIONE POD DB *************
# kubectl create -f deployments/th-prj-db-dep.yml

# UPLOAD SCHEMA ALL'INTERNO DEL POD mysqldb
# 1. si entri nel pod in bash
# kubectl exec -ti mysql-deployment-656c9b55fc-rm6vw -- bash
# 2. si crei una folder: dump
# 3. dalla macchia locale, si effettui l'upload dello schema
# Si noti, che in alcuni casi, all'interno del file di dump, è necessario si sostituisca utf8mb4_0900_ai_ci con utf8mb4_unicode_ci
# kubectl cp db/dump/dump20240307.sql mysql-deployment-68478d458b-s5slh:/dump
# 4. si entri nel pod in bash
# 5. si crei il database:
# mysql -p 
# 20210821Ta!
# create database tarahumara;
# exit;
# mysql -p --database=tarahumara < /dump/dump20240307.sql
# 6. si crei l'utenza dal quale avranno accesso i microservizi:
# mysql -p 
# CREATE USER 'microService'@'%';
# to test > GRANT ALL PRIVILEGES ON tarahumara.* To 'microService'@'%' IDENTIFIED BY '20210821Bp!';
# GRANT ALL PRIVILEGES ON *.* TO 'microService'@'%' IDENTIFIED BY '20240307Tp!';
# exit;
# exit
# # ************* CREAZIONE SERVICE DB *************
# kubectl create -f services/th-prj-mysql-srv.yml

# # ************* CREAZIONE CONFIG MAP MS-ZUUL *************                                                                                                                          
# kubectl create configmap ms-zuul-config --from-literal=springProfile=struct1 --from-literal=MsUserIp=ms-user-gke-service --from-literal=MsAssociationsIp=ms-associations-gke-service --from-literal=MsRunnersIp=ms-runners-gke-service --from-literal=MsRunningsIp=ms-runnings-gke-service
# # ************* CREAZIONE DEPLOYMENT MS-ZUUL *************
# kubectl create -f deployments/th-prj-zuul-dep.yml
# # ************* CREAZIONE SERVICE MS-ZUUL *************
# 1. Creazione external static IP
# REGION=$(gcloud config get-value compute/region)
# gcloud compute addresses create zuul-kube-service-static-ip --region=${REGION}
# Es.
# gcloud compute addresses create zuul-gke-service-static-ip --region=us-central1
#
# Si estragga la lista degli IP generati attraverso il comando: 
# gcloud compute addresses list
#
# 2. Sostituire, all'interno dell'zuul-srv:
# la proprietà loadBalancerIP. Si valorizzi tale proprietà con l'IP genereato.
# to test -> la proprietà loadBalancerSourceRanges, con la subnetmask, dell'ip statico presente all'interno della variabie ingress-kube-service-static-ip. Tale configurazione limita l'accesso al kube-service.
# 3. Si crei l'instanza kube-service
# kubectl create -f services/th-prj-zuul-srv.yml

# # ************* CREAZIONE CONFIG MAP MS-USER *************
# kubectl create configmap ms-user-config --from-literal=springProfile=struct1 --from-literal=MySqlIp=mysql-gke-service --from-literal=ZuulIp=zuul-gke-service
# # ************* CREAZIONE DEPLOYMENT MS-USER *************
# kubectl create -f deployments/th-prj-ms-user-dep.yml
# # ************* CREAZIONE SERVICE MS-USER *************
# 1. Creazione external static IP
# REGION=$(gcloud config get-value compute/region)
# gcloud compute addresses create ms-user-gke-service-static-ip --region=${REGION}
# Es.
# gcloud compute addresses create ms-user-gke-service-static-ip --region=us-central1
#
# Si estragga la lista degli IP generati attraverso il comando: 
# gcloud compute addresses list
#
# 2. Sostituire, all'interno dell'user-srv:
# la proprietà loadBalancerIP. Si valorizzi tale proprietà con l'IP genereato
# --- > TO TEST la proprietà loadBalancerSourceRanges con la subnetmask degli ip statici presenti all'interno della variabie: zuul-kube-service-static-ip, runners-kube-service-static-ip. Tale configurazione limita l'accesso al kube-service.
# 3. Si crei l'instanza ms-user-gke-service
# kubectl create -f services/th-prj-ms-user-srv.yml

# # ************* CREAZIONE CONFIG MAP MS-ASSOCIATIONS *************
# kubectl create configmap ms-associations-config --from-literal=springProfile=zone1 --from-literal=MySqlIp=mysql-gke-service --from-literal=ZuulIp=zuul-gke-service
# # ************* CREAZIONE DEPLOYMENT ASSOCIATIONS *************
# kubectl create -f deployments/th-prj-ms-associations-dep.yml
# # ************* CREAZIONE SERVICE ASSOCIATIONS *************
# 1. Creazione external static IP
# REGION=$(gcloud config get-value compute/region)
# gcloud compute addresses create associations-kube-service-static-ip --region=${REGION}
# Es.
# gcloud compute addresses create associations-kube-service-static-ip --region=us-central1
#
# Si estragga la lista degli IP generati attraverso il comando: 
# gcloud compute addresses list
# 2. Sostituire, all'interno dell'associations-srv:
# la proprietà loadBalancerIP. Si valorizzi tale proprietà con l'IP genereato
# TO TEST----> la proprietà loadBalancerSourceRanges con la subnetmask degli ip statici presenti all'interno della variabie: zuul-kube-service-static-ip, runners-kube-service-static-ip. Tale configurazione limita l'accesso al kube-service.
# 3. Si crei l'instanza associations-service
# kubectl create -f services/th-prj-ms-associations-srv.yml

# # ************* CREAZIONE CONFIG MAP MS-RUNNERS *************
# kubectl create configmap ms-runners-config --from-literal=springProfile=zone1 --from-literal=MySqlIp=mysql-gke-service --from-literal=ZuulIp=zuul-gke-service 
# # ************* CREAZIONE DEPLOYMENT MS-RUNNERS *************
# kubectl create -f deployments/th-prj-ms-runners-dep.yml
# # ************* CREAZIONE SERVICE MS-RUNNERS *************
# 1. Creazione external static IP
# REGION=$(gcloud config get-value compute/region)
# gcloud compute addresses create runners-kube-service-static-ip --region=${REGION}
# Es.
# gcloud compute addresses create runners-kube-service-static-ip --region=us-central1
#
# Si estragga la lista degli IP generati attraverso il comando: 
# gcloud compute addresses list
# 2. Sostituire, all'interno dell'runners-srv:
# la proprietà loadBalancerIP. Si valorizzi tale proprietà con l'IP genereato
# TO TEST----> la proprietà loadBalancerSourceRanges con la subnetmask degli ip statici presenti all'interno della variabie: zuul-kube-service-static-ip, runners-kube-service-static-ip. Tale configurazione limita l'accesso al kube-service.
# 3. Si crei l'instanza associations-service
# kubectl create -f services/th-prj-ms-runners-srv.yml

# # ************* CREAZIONE CONFIG MAP MS-RUNNINGS *************
# kubectl create configmap ms-runnings-config --from-literal=springProfile=zone1 --from-literal=MySqlIp=mysql-gke-service --from-literal=ZuulIp=zuul-gke-service 
# # ************* CREAZIONE DEPLOYMENT MS-RUNNINGS *************
# kubectl create -f deployments/th-prj-ms-runnings-dep.yml
# # ************* CREAZIONE SERVICE MS-RUNNINGS *************
# 1. Creazione external static IP
# REGION=$(gcloud config get-value compute/region)
# gcloud compute addresses create runnings-kube-service-static-ip --region=${REGION}
# Es.
# gcloud compute addresses create runnings-kube-service-static-ip  --region=us-central1
#
# Si estragga la lista degli IP generati attraverso il comando: 
# gcloud compute addresses list
# 2. Sostituire, all'interno dell'runners-srv:
# la proprietà loadBalancerIP. Si valorizzi tale proprietà con l'IP genereato
# TO TEST----> la proprietà loadBalancerSourceRanges con la subnetmask degli ip statici presenti all'interno della variabie: zuul-kube-service-static-ip, runners-kube-service-static-ip. Tale configurazione limita l'accesso al kube-service.
# 3. Si crei l'instanza associations-service
# kubectl create -f services/th-prj-ms-runnings-srv.yml

# # ********** APPLICAZIONE NETWORK POLICIES **********
# Si abilitano le network policies al POD ms-user
# kubectl apply -f netwokrPolicy/ms-user-net-pol.yml 
# Si abilitano le network policies al POD mysql 
# kubectl apply -f netwokrPolicy/th-prj-mysql-net-pol.yml 

# *****************************************************************************
# CREAZIONE SISTEMA AUTOMATIZZATO PER L'ACCENSIONE E LO SPEGNIMENTO DEL CLUSTER
# *****************************************************************************
# TALE PROCEDURA VERIFICA 4 STEP:
# 1. CREAZIONE DEL PUB/SUB, ESSO RAPPRESENTA UN URL CHE SARA' ASSOCIATO ALLA FUNCTION CLOUD PER ESSERE RICHIAMATA
# 2. CREAZIONE DI UNA FUNCTION IN node.js CHE, DATO JSON IN INGRESSO SCALA UN DATO CLUSTER
# 3. CREAZIONE JOB DI ACCENSIONE E SPEGNIMENTO. ESSI, VERRANNO COSI' AVVIATI AUTOMATICAMENTE E RICHIAMERANNO LA FUNCTION
# CHE SCALERA' IL CLUSTER SECONDO QUANTO DESIDERATO.

# # ************* CREAZIONE PUB/SUB *************
# gcloud pubsub topics create scale-gke-event

# # ************* CREAZIONE FUNCTION *************
# SI CREI, ALL'INTERNO DELLA SEZIONE GOOGLE CLOUD FUNCTION UNA FUNCTION SEGUENDO GLI STEP DEFINITI ALL'INTERNO 
# DEGLI SCREENSHOT doc/config/cloud_function/cloud_functio_0 , doc/config/cloud_function/cloud_functio_1
# SI SELEZIONI, COME RUNTIME node.JS 18
# SI SOVRASCRIVA IL FILE index.js CON QUANTO PRESENTE ALL'INTERNO DELLA FOLDER doc/config/cloud_function/index.js
# SI SOVRASCRIVA IL FILE package.json CON QUANTO PRESENTE ALL'INTERNO DELLA FOLDER doc/config/cloud_function/package.json
# SI SETTI, ALL'INTERNO DI GOOGLE CLOUD FUNCTION, L'ENTY POINT IN scalegkePubSub COME PRESENTE IN FIGURA doc/config/cloud_function/cloud_functio_2
# SI ESEGUA IL DEPLOYMENT

# ************* CREAZIONE JOB SCHEDULER ACCENSIONE SPEGNIMENTO *************
# SI ACCEDA ALLA SEZIONE  CLOUD SCHEDULER
# SI CREI IL JOB gke-scale-down E SI DEFINISCA LA PIANIFICAZIONE DESIDERATA. LA FIGURA doc/config/cloud_function/cloud_functio_3 DESCRIVE UN ESEMPIO, QUINDI SI SELEZIONI CONTINUA
# SI CONFIGURI L'ESECUZIONE SECONDO QUANDO DEFINITO IN FIGURA doc/config/cloud_function/cloud_functio_4 E SI POPOLI LA SEZIONE 'Corpo del messaggio' INSERENDO QUINDI
# IL JSON CHE LA FUNCTION CLOUD, PRECEDENTEMENTE REALIZZTA SI ATTENDE (SI INSERISCANO, PER LE DIFFEENTI PROPERTIES GLI OPPORTUNI DATI):
{ 
  "projectId": "base-prj-20240117", 
  "zone": "us-central1-c", 
  "cluster_id": "base-prj-20240117-cls", 
  "node_pool_id": "default-pool", 
  "node_count": 0
}
# SI SALVI

# ************* CREAZIONE JOB SCHEDULER ACCENSIONE ACCENSIONE *************
# SI UTILIZZI LA MEDESIMA PROCEDURA PER LO SPEGNIMENTO, ANDANDO A RIVALORIZZARE LA PIANIFICAZIONE DESIDERATA
# E PORTANDO AD 1 MINIMO IL NUMERO DI NODI DEL CLUSTER QUINDI "node_count": 1

# *****************************************************************************
# ***************************** FINE ******************************************
# *****************************************************************************